{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "70cddb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+-------------+----------+\n",
      "|ID_produit|Nom_produit|Quantite_vendue|Prix_unitaire|Date_vente|\n",
      "+----------+-----------+---------------+-------------+----------+\n",
      "|         1|    Chemise|           10.0|         25.0|2022-01-05|\n",
      "|         2|   Pantalon|            8.0|         35.0|2022-01-06|\n",
      "|         3| Chaussures|           NULL|         50.0|2022-01-07|\n",
      "|         4|    Cravate|           12.0|         15.0|2022-01-08|\n",
      "|         5|       Robe|           15.0|         45.0|2022-01-09|\n",
      "+----------+-----------+---------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ID_produit: integer (nullable = true)\n",
      " |-- Nom_produit: string (nullable = true)\n",
      " |-- Quantite_vendue: double (nullable = true)\n",
      " |-- Prix_unitaire: double (nullable = true)\n",
      " |-- Date_vente: string (nullable = true)\n",
      "\n",
      "+-------+------------------+-----------+------------------+------------------+------------+\n",
      "|summary|        ID_produit|Nom_produit|   Quantite_vendue|     Prix_unitaire|  Date_vente|\n",
      "+-------+------------------+-----------+------------------+------------------+------------+\n",
      "|  count|              5266|       5255|              2594|              2697|        5266|\n",
      "|   mean|  50.7001519179643|       NULL| 10.81129529683887| 54.12290322580648|        NULL|\n",
      "| stddev|29.088180242092857|       NULL|5.5966885644726245|25.663199102855064|        NULL|\n",
      "|    min|                 1| Chaussures|               0.0|             10.04|  01/01/2024|\n",
      "|    max|               100|       Robe|              25.0|             99.93|invalid_date|\n",
      "+-------+------------------+-----------+------------------+------------------+------------+\n",
      "\n",
      "Nombre de lignes : 5266\n",
      "Nombre de colonnes : 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création d'une SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse de données ETL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Chargement des données\n",
    "df = spark.read.csv('./jeu_donnees_etl_5000_lignes.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Affichage des premières lignes\n",
    "df.show(5)  # équivalent de df.head() en Pandas\n",
    "\n",
    "# Informations générales (schéma et types)\n",
    "df.printSchema()\n",
    "\n",
    "# Statistiques descriptives\n",
    "df.describe().show()\n",
    "\n",
    "# Nombre de lignes et de colonnes\n",
    "print(f\"Nombre de lignes : {df.count()}\")\n",
    "print(f\"Nombre de colonnes : {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22e25b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avant suppression : 5266\n",
      "Nombre de lignes après suppression : 4965\n",
      "Nombre de doublons supprimés : 301\n"
     ]
    }
   ],
   "source": [
    "# Nombre de lignes avant suppression\n",
    "nb_lignes_avant = df.count()\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Nombre de lignes après suppression\n",
    "nb_lignes_apres = df.count()\n",
    "\n",
    "# Affichage du résultat\n",
    "print(f\"Nombre de lignes avant suppression : {nb_lignes_avant}\")\n",
    "print(f\"Nombre de lignes après suppression : {nb_lignes_apres}\")\n",
    "print(f\"Nombre de doublons supprimés : {nb_lignes_avant - nb_lignes_apres}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d67291a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de NaN dans la colonne 'Quantite_vendue' est : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Supprimer les lignes où 'Quantite_vendue' est manquante\n",
    "df = df.filter(col(\"Quantite_vendue\").isNotNull())\n",
    "\n",
    "# Convertir 'Quantite_vendue' en entier (si elle ne l'est pas déjà)\n",
    "df = df.withColumn(\"Quantite_vendue\", col(\"Quantite_vendue\").cast(\"int\"))\n",
    "\n",
    "# Vérifier le nombre de valeurs nulles dans la colonne 'Quantite_vendue'\n",
    "nb_nan = df.select(count(when(col(\"Quantite_vendue\").isNull(), True)).alias(\"nb_nan\")).collect()[0][\"nb_nan\"]\n",
    "print(f\"Le nombre de NaN dans la colonne 'Quantite_vendue' est : {nb_nan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c18bb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_produit: integer (nullable = true)\n",
      " |-- Nom_produit: string (nullable = true)\n",
      " |-- Quantite_vendue: integer (nullable = true)\n",
      " |-- Prix_unitaire: double (nullable = true)\n",
      " |-- Date_vente: string (nullable = true)\n",
      "\n",
      "+----------+-----------+---------------+-------------+------------+\n",
      "|ID_produit|Nom_produit|Quantite_vendue|Prix_unitaire|  Date_vente|\n",
      "+----------+-----------+---------------+-------------+------------+\n",
      "|        37|    Chemise|             10|         25.0|  2022-01-05|\n",
      "|        65|       Robe|             15|         45.0|  2022-01-09|\n",
      "|        64|    Cravate|             18|        87.41|  31/10/2024|\n",
      "|        56|       Robe|              8|        23.35|invalid_date|\n",
      "|        42|     Montre|              4|        40.91|invalid_date|\n",
      "+----------+-----------+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc164b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Fonction pour appliquer la winsorisation sur une colonne\n",
    "def winsorize_column(df, column_name):\n",
    "    # Calcul des quartiles\n",
    "    q1, q3 = df.approxQuantile(column_name, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "\n",
    "    # Clipping des valeurs aberrantes\n",
    "    return df.withColumn(\n",
    "        column_name,\n",
    "        when(col(column_name) < lower, lower)\n",
    "        .when(col(column_name) > upper, upper)\n",
    "        .otherwise(col(column_name))\n",
    "    )\n",
    "\n",
    "# Appliquer sur les deux colonnes\n",
    "df = winsorize_column(df, \"Quantite_vendue\")\n",
    "df = winsorize_column(df, \"Prix_unitaire\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d240df81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doublons_restants : 6\n",
      "valeurs_manquantes : {'ID_produit': 0, 'Nom_produit': 10, 'Quantite_vendue': 0, 'Prix_unitaire': 1195, 'Date_vente': 0}\n",
      "quantite_negative : 0\n",
      "prix_negative : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Vérifier les doublons restants (lignes identiques)\n",
    "doublons_restants = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "# Compter les valeurs manquantes pour chaque colonne\n",
    "valeurs_manquantes = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Vérifier les valeurs négatives\n",
    "quantite_negative = df.filter(col(\"Quantite_vendue\") < 0).count()\n",
    "prix_negative = df.filter(col(\"Prix_unitaire\") < 0).count()\n",
    "\n",
    "# Résumé sous forme de dictionnaire\n",
    "validation = {\n",
    "    \"doublons_restants\": doublons_restants,\n",
    "    \"valeurs_manquantes\": valeurs_manquantes,\n",
    "    \"quantite_negative\": quantite_negative,\n",
    "    \"prix_negative\": prix_negative\n",
    "}\n",
    "\n",
    "# Afficher les résultats\n",
    "for cle, valeur in validation.items():\n",
    "    print(f\"{cle} : {valeur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cf4d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doublons_restants : 0\n",
      "valeurs_manquantes : {'ID_produit': 0, 'Nom_produit': 0, 'Quantite_vendue': 0, 'Prix_unitaire': 0, 'Date_vente': 0}\n",
      "quantite_negative : 0\n",
      "prix_negative : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# 1. Supprimer les lignes où 'Nom_produit' ou 'Prix_unitaire' sont nulles\n",
    "df = df.dropna(subset=['Nom_produit', 'Prix_unitaire'])\n",
    "\n",
    "# 2. Validation des données\n",
    "\n",
    "# Doublons restants\n",
    "doublons_restants = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "# Valeurs manquantes par colonne\n",
    "valeurs_manquantes = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Quantités négatives\n",
    "quantite_negative = df.filter(col(\"Quantite_vendue\") < 0).count()\n",
    "\n",
    "# Prix négatifs\n",
    "prix_negative = df.filter(col(\"Prix_unitaire\") < 0).count()\n",
    "\n",
    "# Résumé\n",
    "validation = {\n",
    "    \"doublons_restants\": doublons_restants,\n",
    "    \"valeurs_manquantes\": valeurs_manquantes,\n",
    "    \"quantite_negative\": quantite_negative,\n",
    "    \"prix_negative\": prix_negative\n",
    "}\n",
    "\n",
    "# Affichage\n",
    "for cle, valeur in validation.items():\n",
    "    print(f\"{cle} : {valeur}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed154031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "try:\n",
    "    df = df.withColumn(\"Valeur_totale\", col(\"Quantite_vendue\") * col(\"Prix_unitaire\"))\n",
    "except Exception as e:\n",
    "    logging.error(f\"Erreur lors du calcul de la colonne 'Valeur_totale': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5eeacee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"Montant_total\", col(\"Quantite_vendue\") * col(\"Prix_unitaire\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "36a2ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min as spark_min, max as spark_max, col\n",
    "\n",
    "# Calcul des min et max avec alias différents\n",
    "stats = df.agg(\n",
    "    spark_min(\"Montant_total\").alias(\"min_montant\"),\n",
    "    spark_max(\"Montant_total\").alias(\"max_montant\")\n",
    ").collect()[0]\n",
    "\n",
    "montant_min = stats[\"min_montant\"]\n",
    "montant_max = stats[\"max_montant\"]\n",
    "\n",
    "# Création de la colonne normalisée\n",
    "df = df.withColumn(\n",
    "    \"Montant_total_normalise\",\n",
    "    (col(\"Montant_total\") - montant_min) / (montant_max - montant_min)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4433bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "df_agg = df.groupBy(\"Nom_produit\") \\\n",
    "           .agg(spark_sum(\"Valeur_totale\").alias(\"Vente_totale\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc0afb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitements effectués :\n",
      "['Suppression des doublons', 'Imputation des valeurs manquantes avec la médiane', 'Winsorisation des valeurs aberrantes', \"Ajout de la colonne 'Valeur_totale'\", 'Agrégation des ventes par produit']\n",
      "\n",
      "Validation finale :\n",
      "{'doublons_restants': 0, 'valeurs_manquantes': {'ID_produit': 0, 'Nom_produit': 0, 'Quantite_vendue': 0, 'Prix_unitaire': 0, 'Date_vente': 0, 'Valeur_totale': 0, 'Montant_total': 0, 'Montant_total_normalise': 0}, 'quantite_negative': 0, 'prix_negative': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Recalcul des validations finales\n",
    "\n",
    "doublons_restants = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "valeurs_manquantes = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "quantite_negative = df.filter(col(\"Quantite_vendue\") < 0).count()\n",
    "prix_negative = df.filter(col(\"Prix_unitaire\") < 0).count()\n",
    "\n",
    "documentation_resume = {\n",
    "    \"Traitements effectués\": [\n",
    "        \"Suppression des doublons\",\n",
    "        \"Imputation des valeurs manquantes avec la médiane\",\n",
    "        \"Winsorisation des valeurs aberrantes\",\n",
    "        \"Ajout de la colonne 'Valeur_totale'\",\n",
    "        \"Agrégation des ventes par produit\"\n",
    "    ],\n",
    "    \"Validation finale\": {\n",
    "        \"doublons_restants\": doublons_restants,\n",
    "        \"valeurs_manquantes\": valeurs_manquantes,\n",
    "        \"quantite_negative\": quantite_negative,\n",
    "        \"prix_negative\": prix_negative\n",
    "    }\n",
    "}\n",
    "\n",
    "# Affichage lisible\n",
    "for cle, valeur in documentation_resume.items():\n",
    "    print(f\"{cle} :\")\n",
    "    print( valeur)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "016e23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+-------------+------------+------------------+------------------+-----------------------+\n",
      "|ID_produit|Nom_produit|Quantite_vendue|Prix_unitaire|  Date_vente|     Valeur_totale|     Montant_total|Montant_total_normalise|\n",
      "+----------+-----------+---------------+-------------+------------+------------------+------------------+-----------------------+\n",
      "|        37|    Chemise|           10.0|         25.0|  2022-01-05|             250.0|             250.0|    0.13262669828487156|\n",
      "|        65|       Robe|           15.0|         45.0|  2022-01-09|             675.0|             675.0|     0.3580920853691532|\n",
      "|        64|    Cravate|           18.0|        87.41|  31/10/2024|1573.3799999999999|1573.3799999999999|     0.8346887781898048|\n",
      "|        56|       Robe|            8.0|        23.35|invalid_date|             186.8|             186.8|    0.09909866895845602|\n",
      "|        42|     Montre|            4.0|        40.91|invalid_date|            163.64|            163.64|    0.08681213162934552|\n",
      "+----------+-----------+---------------+-------------+------------+------------------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)  # Affiche les 5 premières lignes sous forme tabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a80fcfcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1594.csv.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m dossier_sauvegarde = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msauvegardes_pyspark/donnees_nettoyees_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Sauvegarder le DataFrame au format CSV (PySpark crée un dossier avec plusieurs fichiers CSV)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdossier_sauvegarde\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDonnées sauvegardées dans le dossier : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdossier_sauvegarde\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   1847\u001b[39m     compression=compression,\n\u001b[32m   1848\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1862\u001b[39m     lineSep=lineSep,\n\u001b[32m   1863\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1594.csv.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Créer un dossier de sauvegarde s'il n'existe pas\n",
    "os.makedirs(\"sauvegardes\", exist_ok=True)\n",
    "\n",
    "# Générer un nom de dossier avec la date et l'heure actuelles\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "dossier_sauvegarde = f\"sauvegardes_pyspark/donnees_nettoyees_{timestamp}\"\n",
    "\n",
    "# Sauvegarder le DataFrame au format CSV (PySpark crée un dossier avec plusieurs fichiers CSV)\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(dossier_sauvegarde)\n",
    "\n",
    "print(f\"Données sauvegardées dans le dossier : {dossier_sauvegarde}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b0f4549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_produit: integer (nullable = true)\n",
      " |-- Nom_produit: string (nullable = true)\n",
      " |-- Quantite_vendue: double (nullable = true)\n",
      " |-- Prix_unitaire: double (nullable = true)\n",
      " |-- Date_vente: string (nullable = true)\n",
      " |-- Valeur_totale: double (nullable = true)\n",
      " |-- Montant_total: double (nullable = true)\n",
      " |-- Montant_total_normalise: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
